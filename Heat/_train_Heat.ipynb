{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangdl/anaconda3/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os import path\n",
    "sys.path.append(path.dirname(path.dirname(path.abspath('__file__'))))\n",
    "from Heat import *\n",
    "from Drawer import Drawer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best save at Epoch 1, lr = 0.001, Epoch = 1\n",
      "-- Best loss = 0.4326525331. At Epoch 1\n",
      "\n",
      "Epoch 1, lr = 0.001\n",
      "-- Current loss = 0.4326525331\n",
      "-- Best loss = 0.4326525331. At Epoch 1\n",
      "\n",
      "Best save at Epoch 2, lr = 0.001, Epoch = 2\n",
      "-- Best loss = 0.4002578259. At Epoch 2\n",
      "\n",
      "Epoch 2, lr = 0.001\n",
      "-- Current loss = 0.4002578259\n",
      "-- Best loss = 0.4002578259. At Epoch 2\n",
      "\n",
      "Best save at Epoch 3, lr = 0.001, Epoch = 3\n",
      "-- Best loss = 0.3718365431. At Epoch 3\n",
      "\n",
      "Epoch 3, lr = 0.001\n",
      "-- Current loss = 0.3718365431\n",
      "-- Best loss = 0.3718365431. At Epoch 3\n",
      "\n",
      "Best save at Epoch 4, lr = 0.001, Epoch = 4\n",
      "-- Best loss = 0.347453922. At Epoch 4\n",
      "\n",
      "Epoch 4, lr = 0.001\n",
      "-- Current loss = 0.347453922\n",
      "-- Best loss = 0.347453922. At Epoch 4\n",
      "\n",
      "Best save at Epoch 5, lr = 0.001, Epoch = 5\n",
      "-- Best loss = 0.3273808658. At Epoch 5\n",
      "\n",
      "Epoch 5, lr = 0.001\n",
      "-- Current loss = 0.3273808658\n",
      "-- Best loss = 0.3273808658. At Epoch 5\n",
      "\n",
      "Best save at Epoch 6, lr = 0.001, Epoch = 6\n",
      "-- Best loss = 0.312081635. At Epoch 6\n",
      "\n",
      "Epoch 6, lr = 0.001\n",
      "-- Current loss = 0.312081635\n",
      "-- Best loss = 0.312081635. At Epoch 6\n",
      "\n",
      "Best save at Epoch 7, lr = 0.001, Epoch = 7\n",
      "-- Best loss = 0.3020687699. At Epoch 7\n",
      "\n",
      "Epoch 7, lr = 0.001\n",
      "-- Current loss = 0.3020687699\n",
      "-- Best loss = 0.3020687699. At Epoch 7\n",
      "\n",
      "Best save at Epoch 8, lr = 0.001, Epoch = 8\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 8, lr = 0.001\n",
      "-- Current loss = 0.2976388335\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 9, lr = 0.001\n",
      "-- Current loss = 0.298440516\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 10, lr = 0.001\n",
      "-- Current loss = 0.302865386\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 11, lr = 0.001\n",
      "-- Current loss = 0.3080612421\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 12, lr = 0.001\n",
      "-- Current loss = 0.3115634322\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 13, lr = 0.001\n",
      "-- Current loss = 0.3124856949\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 14, lr = 0.001\n",
      "-- Current loss = 0.3111647964\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 15, lr = 0.001\n",
      "-- Current loss = 0.3084506691\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 16, lr = 0.001\n",
      "-- Current loss = 0.3052330315\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 17, lr = 0.001\n",
      "-- Current loss = 0.3022129238\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 18, lr = 0.001\n",
      "-- Current loss = 0.2998282015\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Epoch 19, lr = 0.001\n",
      "-- Current loss = 0.2982652485\n",
      "-- Best loss = 0.2976388335. At Epoch 8\n",
      "\n",
      "Best save at Epoch 20, lr = 0.001, Epoch = 20\n",
      "-- Best loss = 0.2975129187. At Epoch 20\n",
      "\n",
      "Epoch 20, lr = 0.001\n",
      "-- Current loss = 0.2975129187\n",
      "-- Best loss = 0.2975129187. At Epoch 20\n",
      "\n",
      "Best save at Epoch 21, lr = 0.001, Epoch = 21\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 21, lr = 0.001\n",
      "-- Current loss = 0.2974294424\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 22, lr = 0.001\n",
      "-- Current loss = 0.2978067398\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 23, lr = 0.001\n",
      "-- Current loss = 0.2984224558\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 24, lr = 0.001\n",
      "-- Current loss = 0.2990778685\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 25, lr = 0.001\n",
      "-- Current loss = 0.2996216416\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 26, lr = 0.001\n",
      "-- Current loss = 0.2999596\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 27, lr = 0.001\n",
      "-- Current loss = 0.3000534773\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 28, lr = 0.001\n",
      "-- Current loss = 0.299911797\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 29, lr = 0.001\n",
      "-- Current loss = 0.2995772064\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 30, lr = 0.001\n",
      "-- Current loss = 0.299112916\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 31, lr = 0.001\n",
      "-- Current loss = 0.2985896468\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 32, lr = 0.001\n",
      "-- Current loss = 0.2980745435\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Epoch 33, lr = 0.001\n",
      "-- Current loss = 0.2976219058\n",
      "-- Best loss = 0.2974294424. At Epoch 21\n",
      "\n",
      "Best save at Epoch 34, lr = 0.001, Epoch = 34\n",
      "-- Best loss = 0.2972667813. At Epoch 34\n",
      "\n",
      "Epoch 34, lr = 0.001\n",
      "-- Current loss = 0.2972667813\n",
      "-- Best loss = 0.2972667813. At Epoch 34\n",
      "\n",
      "Best save at Epoch 35, lr = 0.001, Epoch = 35\n",
      "-- Best loss = 0.2970212698. At Epoch 35\n",
      "\n",
      "Epoch 35, lr = 0.001\n",
      "-- Current loss = 0.2970212698\n",
      "-- Best loss = 0.2970212698. At Epoch 35\n",
      "\n",
      "Best save at Epoch 36, lr = 0.001, Epoch = 36\n",
      "-- Best loss = 0.2968740761. At Epoch 36\n",
      "\n",
      "Epoch 36, lr = 0.001\n",
      "-- Current loss = 0.2968740761\n",
      "-- Best loss = 0.2968740761. At Epoch 36\n",
      "\n",
      "Best save at Epoch 37, lr = 0.001, Epoch = 37\n",
      "-- Best loss = 0.2967929542. At Epoch 37\n",
      "\n",
      "Epoch 37, lr = 0.001\n",
      "-- Current loss = 0.2967929542\n",
      "-- Best loss = 0.2967929542. At Epoch 37\n",
      "\n",
      "Best save at Epoch 38, lr = 0.001, Epoch = 38\n",
      "-- Best loss = 0.2967291474. At Epoch 38\n",
      "\n",
      "Epoch 38, lr = 0.001\n",
      "-- Current loss = 0.2967291474\n",
      "-- Best loss = 0.2967291474. At Epoch 38\n",
      "\n",
      "Best save at Epoch 39, lr = 0.001, Epoch = 39\n",
      "-- Best loss = 0.296623528. At Epoch 39\n",
      "\n",
      "Epoch 39, lr = 0.001\n",
      "-- Current loss = 0.296623528\n",
      "-- Best loss = 0.296623528. At Epoch 39\n",
      "\n",
      "Best save at Epoch 40, lr = 0.001, Epoch = 40\n",
      "-- Best loss = 0.2964117527. At Epoch 40\n",
      "\n",
      "Epoch 40, lr = 0.001\n",
      "-- Current loss = 0.2964117527\n",
      "-- Best loss = 0.2964117527. At Epoch 40\n",
      "\n",
      "Best save at Epoch 41, lr = 0.001, Epoch = 41\n",
      "-- Best loss = 0.2960276604. At Epoch 41\n",
      "\n",
      "Epoch 41, lr = 0.001\n",
      "-- Current loss = 0.2960276604\n",
      "-- Best loss = 0.2960276604. At Epoch 41\n",
      "\n",
      "Best save at Epoch 42, lr = 0.001, Epoch = 42\n",
      "-- Best loss = 0.2954037488. At Epoch 42\n",
      "\n",
      "Epoch 42, lr = 0.001\n",
      "-- Current loss = 0.2954037488\n",
      "-- Best loss = 0.2954037488. At Epoch 42\n",
      "\n",
      "Best save at Epoch 43, lr = 0.001, Epoch = 43\n",
      "-- Best loss = 0.2944677472. At Epoch 43\n",
      "\n",
      "Epoch 43, lr = 0.001\n",
      "-- Current loss = 0.2944677472\n",
      "-- Best loss = 0.2944677472. At Epoch 43\n",
      "\n",
      "Best save at Epoch 44, lr = 0.001, Epoch = 44\n",
      "-- Best loss = 0.2931368053. At Epoch 44\n",
      "\n",
      "Epoch 44, lr = 0.001\n",
      "-- Current loss = 0.2931368053\n",
      "-- Best loss = 0.2931368053. At Epoch 44\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m net \u001b[39m=\u001b[39m Net(pde_size \u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m), \n\u001b[1;32m      5\u001b[0m           shape \u001b[39m=\u001b[39m (\u001b[39m16\u001b[39m, \u001b[39m32\u001b[39m), \n\u001b[1;32m      6\u001b[0m           data \u001b[39m=\u001b[39m heat\u001b[39m.\u001b[39mdata_generator(),\n\u001b[1;32m      7\u001b[0m           loadFile \u001b[39m=\u001b[39m nil, lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m)\n\u001b[1;32m     10\u001b[0m heat\u001b[39m.\u001b[39msetNet(net)\n\u001b[0;32m---> 11\u001b[0m heat\u001b[39m.\u001b[39;49mtrain(\u001b[39m100000\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Neural-Network-PDE/PDE_Square.py:29\u001b[0m, in \u001b[0;36mPDE_Square.train\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, epoch):\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet\u001b[39m.\u001b[39;49mtrain(epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss)\n",
      "File \u001b[0;32m~/Neural-Network-PDE/Net.py:209\u001b[0m, in \u001b[0;36mNet.train\u001b[0;34m(self, epoch, loss_fn)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnt_Epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[1;32m    208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madam\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madam\u001b[39m.\u001b[39;49mstep(loss_fn)\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msched\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_current)\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/optim/adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 121\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    124\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Neural-Network-PDE/Heat.py:24\u001b[0m, in \u001b[0;36mHeat.loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m X\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[1;32m     23\u001b[0m U \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(X)\n\u001b[0;32m---> 24\u001b[0m dU \u001b[39m=\u001b[39m grad(U, X, tc\u001b[39m.\u001b[39;49mones_like(U), \u001b[39mTrue\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m pt \u001b[39m=\u001b[39m dU[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m     26\u001b[0m px \u001b[39m=\u001b[39m dU[:, \u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:269\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    267\u001b[0m overridable_args \u001b[39m=\u001b[39m t_outputs \u001b[39m+\u001b[39m t_inputs\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    270\u001b[0m         grad,\n\u001b[1;32m    271\u001b[0m         overridable_args,\n\u001b[1;32m    272\u001b[0m         t_outputs,\n\u001b[1;32m    273\u001b[0m         t_inputs,\n\u001b[1;32m    274\u001b[0m         grad_outputs\u001b[39m=\u001b[39;49mgrad_outputs,\n\u001b[1;32m    275\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    276\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    277\u001b[0m         only_inputs\u001b[39m=\u001b[39;49monly_inputs,\n\u001b[1;32m    278\u001b[0m         allow_unused\u001b[39m=\u001b[39;49mallow_unused,\n\u001b[1;32m    279\u001b[0m         is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m only_inputs:\n\u001b[1;32m    283\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1531\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1536\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best = 'models/best.pt'\n",
    "nil = ''\n",
    "heat = Heat(t = (0, 1), x = (0, 1), N = 10000)\n",
    "net = Net(pde_size = (2, 1), \n",
    "          shape = (16, 32), \n",
    "          data = heat.data_generator(),\n",
    "          loadFile = nil, lr = 1e-3)\n",
    "\n",
    "\n",
    "heat.setNet(net)\n",
    "heat.train(100000)\n",
    "print(\"Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
