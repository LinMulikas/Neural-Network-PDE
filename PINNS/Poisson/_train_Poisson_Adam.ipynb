{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Poisson import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 1.8493344784, lr = 0.01.\n",
      "- Best dict saved at epoch 1.\n",
      "  With best loss: 1.8493344784.\n",
      "\n",
      "Epoch 2, loss = 1.531036377, lr = 0.01.\n",
      "- Best dict saved at epoch 2.\n",
      "  With best loss: 1.5310363770.\n",
      "\n",
      "Epoch 3, loss = 1.2317923307, lr = 0.01.\n",
      "- Best dict saved at epoch 3.\n",
      "  With best loss: 1.2317923307.\n",
      "\n",
      "Epoch 4, loss = 0.9347529411, lr = 0.01.\n",
      "- Best dict saved at epoch 4.\n",
      "  With best loss: 0.9347529411.\n",
      "\n",
      "Epoch 5, loss = 0.6537191868, lr = 0.01.\n",
      "- Best dict saved at epoch 5.\n",
      "  With best loss: 0.6537191868.\n",
      "\n",
      "Epoch 6, loss = 0.4265216291, lr = 0.01.\n",
      "- Best dict saved at epoch 6.\n",
      "  With best loss: 0.4265216291.\n",
      "\n",
      "Epoch 7, loss = 0.2896195054, lr = 0.01.\n",
      "- Best dict saved at epoch 7.\n",
      "  With best loss: 0.2896195054.\n",
      "\n",
      "Epoch 8, loss = 0.2542138398, lr = 0.01.\n",
      "- Best dict saved at epoch 8.\n",
      "  With best loss: 0.2542138398.\n",
      "\n",
      "Epoch 9, loss = 0.295764029, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 10, loss = 0.3613539934, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 11, loss = 0.4057578444, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 12, loss = 0.4150292277, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 13, loss = 0.3951807618, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 14, loss = 0.3585782051, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 15, loss = 0.3177395165, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 16, loss = 0.2828737795, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 17, loss = 0.2606695592, lr = 0.01.\n",
      "  Best loss: 0.2542138398 at epoch 8.\n",
      "Epoch 18, loss = 0.2535737157, lr = 0.01.\n",
      "- Best dict saved at epoch 18.\n",
      "  With best loss: 0.2535737157.\n",
      "\n",
      "Epoch 19, loss = 0.2596190274, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 20, loss = 0.2732076645, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 21, loss = 0.2872729003, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 22, loss = 0.2962139845, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 23, loss = 0.2977133691, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 24, loss = 0.2925235629, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 25, loss = 0.2831681967, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 26, loss = 0.2726151049, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 27, loss = 0.2633565366, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 28, loss = 0.2569471896, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 29, loss = 0.2539100051, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 30, loss = 0.2538942695, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 31, loss = 0.2559694946, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 32, loss = 0.2589635551, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 33, loss = 0.2617730498, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 34, loss = 0.2635928392, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 35, loss = 0.2640354037, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 36, loss = 0.2631302476, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 37, loss = 0.261231333, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 38, loss = 0.2588677108, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 39, loss = 0.2565859258, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 40, loss = 0.254819721, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 41, loss = 0.2538102567, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 42, loss = 0.2535854876, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 43, loss = 0.2539925873, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 44, loss = 0.2547653317, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 45, loss = 0.2556056082, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 46, loss = 0.2562574148, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 47, loss = 0.2565584183, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 48, loss = 0.2564604878, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 49, loss = 0.2560218275, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 50, loss = 0.2553745508, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n",
      "Epoch 51, loss = 0.2546815574, lr = 0.01.\n",
      "  Best loss: 0.2535737157 at epoch 18.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[39m=\u001b[39m PDENN(\n\u001b[1;32m      2\u001b[0m     input_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, hidden_depth\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, hidden_size\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m, \n\u001b[1;32m      3\u001b[0m     lr \u001b[39m=\u001b[39m \u001b[39m1e-2\u001b[39m)\n\u001b[1;32m      5\u001b[0m eq \u001b[39m=\u001b[39m Poisson((\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), \u001b[39m800\u001b[39m, load_best\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, net\u001b[39m=\u001b[39mnet)\n\u001b[0;32m----> 6\u001b[0m eq\u001b[39m.\u001b[39;49mtrain(\u001b[39m20000\u001b[39;49m)\n",
      "File \u001b[0;32m~/Github/Neural-Network-PDE/PINNS/PDE2D.py:204\u001b[0m, in \u001b[0;36mPDE2D.train\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39mraise\u001b[39;00m(\u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo instance of method.\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    203\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mcnt_Epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):   \n\u001b[0;32m--> 204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss)\n\u001b[1;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_lr):\n\u001b[1;32m    206\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39msched\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mloss_tensor)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.16/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.16/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.16/lib/python3.9/site-packages/torch/optim/adam.py:118\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 118\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    121\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Github/Neural-Network-PDE/PINNS/PDE2D.py:353\u001b[0m, in \u001b[0;36mPDE2D.loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39m#? Calculate the Loss\u001b[39;00m\n\u001b[1;32m    352\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculateLoss()\n\u001b[0;32m--> 353\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    355\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mloss_tensor \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    356\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mcnt_Epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet\u001b[39m.\u001b[39mcnt_Epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.16/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.16/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = PDENN(\n",
    "    input_size=2, output_size=1, hidden_depth=10, hidden_size=20, \n",
    "    lr = 1e-2)\n",
    "\n",
    "eq = Poisson((0, 1), (0, 1), 1000, load_best=False, net=net)\n",
    "eq.train(20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
